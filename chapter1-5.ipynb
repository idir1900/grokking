{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "49e6be29-1bbd-40a7-ae60-21f856e248fe",
      "cell_type": "code",
      "source": "print(\"idir\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "idir\n"
        }
      ],
      "execution_count": 1
    },
    {
      "id": "c09525a2-6f33-42e8-8576-77b5e1084310",
      "cell_type": "code",
      "source": "def neural_network(weight, input):\n    return input * weight",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "id": "aac7d2e2-359a-4e17-b9e7-4c46f0cd5a88",
      "cell_type": "code",
      "source": "neural_network(0.1, 5)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.5"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3
    },
    {
      "id": "4bb95ff4-18b7-4cae-9f0c-4d862e222a8c",
      "cell_type": "code",
      "source": "def elementwise_multiplication(vec_a, vec_b):\n    return [vec_a * vec_b for vec_a, vec_b in zip(vec_a,vec_b)]\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "id": "952757a8-4ba6-4ca5-bcc6-e51e1d4eac01",
      "cell_type": "code",
      "source": "vectora = [1,2,3]\nvectorb = [0.1,0.2,0.85]\nelementwise_multiplication(vectora, vectorb)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[0.1, 0.4, 2.55]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5
    },
    {
      "id": "d726263b-68c5-4801-90a9-26ee23100b8a",
      "cell_type": "code",
      "source": "def elementwise_addition(vec_a, vec_b):\n    return [vec_a + vec_b for vec_a, vec_b in zip(vec_a,vec_b)]\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "id": "59f68e1d-2fbd-40e8-a93a-7b96f57207c2",
      "cell_type": "code",
      "source": "vectora = [1,2,3]\nvectorb = [0.1,0.2,0.85]\nelementwise_addition(vectora, vectorb)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[1.1, 2.2, 3.85]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7
    },
    {
      "id": "266247e9-6d88-45ad-855c-b2f37a1ae7b3",
      "cell_type": "code",
      "source": "def sum_of_vector(vec_a):\n    sum = 0\n    for i in range(len(vec_a)):\n        sum = sum + vec_a[i]\n    return sum",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "id": "c542c103-7f96-4d8a-838a-499d5a73f0b3",
      "cell_type": "code",
      "source": "sum_of_vector(vectora)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "6"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9
    },
    {
      "id": "82b86259-d972-449c-a309-8567175942b3",
      "cell_type": "code",
      "source": "def sum_vector_avg(vec_a):\n    return sum_of_vector(vec_a)/len(vec_a)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "id": "8bec6a85-339f-4a70-a45f-05c4a0717d5a",
      "cell_type": "code",
      "source": "sum_vector_avg(vectora)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "2.0"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11
    },
    {
      "id": "31b84083-a389-4929-8636-bd4256742053",
      "cell_type": "raw",
      "source": "",
      "metadata": {}
    },
    {
      "id": "8573cea9-588a-40f9-8da8-8f3938a6f750",
      "cell_type": "code",
      "source": "import numpy as np\nw = np.array([[0.1, 0.1, -0.3],\n              [0.1, 0.2, 0.0],\n              [0.0, 1.3, 0.1]])\n\n\nw2 = np.array([[0.4, -0.1, -0.3],\n              [0.2, 0.1, 0.0],\n              [0.1, 1.0, -0.1]])\n\ni = np.array([8.5, 0.65, 1.2])\n\n\ndef vect_mat_mul(input, weights):\n    return np.matmul(weights, input)\n\ndef neural_network (input, weights):\n    return vect_mat_mul(input,weights)\n\npred = neural_network(i, w)\nprint(pred)\n\npred = neural_network(pred, w2)\nprint(pred)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[0.555 0.98  0.965]\n[-0.1655  0.209   0.939 ]\n"
        }
      ],
      "execution_count": 13
    },
    {
      "id": "6ace75e4-a96a-4840-8ecd-4a7841d2b649",
      "cell_type": "markdown",
      "source": "",
      "metadata": {}
    },
    {
      "id": "42feb06a-fe21-47ec-85ec-740e2c70056b",
      "cell_type": "markdown",
      "source": "### the simplest form of neural network\n    the simplest way is to add a step that we call  lr  , we modify the weight by this amount in the positive and negative direction and see in what direction the error is smaller.",
      "metadata": {}
    },
    {
      "id": "af6b15b5-815c-4dfa-8517-4a45b6ec4e19",
      "cell_type": "markdown",
      "source": "def new_network():\n    lr = 0.01\n    weight = 0.1\n    input = 8.5\n    output = 1\n    prediction = weight * input # norml prediction\n    print(\"the predicition is \" +str(prediction) )\n    error = (prediction - output) ** 2 # to make the error allways positive also to make the error greater\n    print(\"the error is \"+str(error))\n    while (error > 0.001):\n        pred_low = (weight-lr) * input\n        print(pred_low)\n        pred_high = (weight + lr) * input\n        print(pred_high)\n        error_low = (pred_low - output) ** 2\n        error_high = (pred_high - output) ** 2\n        if (error_low <= error_high):\n            weight = weight - lr\n            error = error_low\n        else:\n            weight += lr\n            error = error_high\n    print(\"the error now is \" + str(error))\n    print(\"the weight now is \" + str(weight))\n\n        ",
      "metadata": {}
    },
    {
      "id": "40911623-d8ba-40b7-bcb1-d8a1050f3100",
      "cell_type": "code",
      "source": "new_network()",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "the predicition is 0.8500000000000001\nthe error is 0.022499999999999975\n0.7650000000000001\n0.935\n0.8500000000000001\n1.02\nthe error now is 0.0004000000000000007\nthe weight now is 0.12\n"
        }
      ],
      "execution_count": 2
    },
    {
      "id": "132ea19b-f271-4ed2-8d3b-c0807b511817",
      "cell_type": "markdown",
      "source": "### hot and cold learning\n    we test a bigger weight and a smaller weight and we see wich one is lessening the error \n#### problem with this methode\n    you have to predict multiple times to make a single knob weight update \n    second problem is that if the step of correction is not small enough the prediction would oscilate arround the correct value and never acheive it \n    so it will be an undless loop",
      "metadata": {}
    },
    {
      "id": "00573eb3-7747-4b28-85b0-995d697f342d",
      "cell_type": "markdown",
      "source": "### Calculation both direction and amount",
      "metadata": {}
    },
    {
      "id": "2fae299a-bbc2-4b9b-b432-c3db504b4ad2",
      "cell_type": "code",
      "source": "weight = 0.5\ngoal_pred =0.8 \ninput = 0.5\n\nfor i in range(20):\n    pred = weight * input\n    error = (pred - goal_pred) ** 2\n    direction_amount = (pred - goal_pred) * input\n    weight = weight - direction_amount\n\n    print(\"Error:\" + str(error) + \"prediition:\" + str (pred))",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Error:0.30250000000000005prediition:0.25\nError:0.17015625000000004prediition:0.3875\nError:0.095712890625prediition:0.49062500000000003\nError:0.05383850097656251prediition:0.56796875\nError:0.03028415679931642prediition:0.6259765625\nError:0.0170348381996155prediition:0.669482421875\nError:0.00958209648728372prediition:0.70211181640625\nError:0.005389929274097089prediition:0.7265838623046875\nError:0.0030318352166796153prediition:0.7449378967285156\nError:0.0017054073093822882prediition:0.7587034225463867\nError:0.0009592916115275371prediition:0.76902756690979\nError:0.0005396015314842384prediition:0.7767706751823426\nError:0.000303525861459885prediition:0.7825780063867569\nError:0.00017073329707118678prediition:0.7869335047900676\nError:9.603747960254256e-05prediition:0.7902001285925507\nError:5.402108227642978e-05prediition:0.7926500964444131\nError:3.038685878049206e-05prediition:0.7944875723333098\nError:1.7092608064027242e-05prediition:0.7958656792499823\nError:9.614592036015323e-06prediition:0.7968992594374867\nError:5.408208020258491e-06prediition:0.7976744445781151\n"
        }
      ],
      "execution_count": 5
    },
    {
      "id": "dad4d6a4-64a5-4570-b252-9f6f5590d9f2",
      "cell_type": "markdown",
      "source": "",
      "metadata": {}
    },
    {
      "id": "20238b2e-afa0-4ced-9722-61bb2e111e3d",
      "cell_type": "markdown",
      "source": "what is we see is the **gradient descent**  it allows us to calculate the direction and the amount of the correction \nsometimes the gradient is too slow so the learning is slow and it can be bigger so the learning is unstable ",
      "metadata": {}
    },
    {
      "id": "4f0701d5-fabf-4289-a378-0c16b1f852ec",
      "cell_type": "markdown",
      "source": "# üß† What is a Gradient?\n\nThe gradient is **not just the difference between the prediction and the output** ‚Äî  \nit‚Äôs the **rate of change of the loss** with respect to each model parameter (like a weight).\n\nFormally, for a weight \\(w\\):\n\n$$\n\\text{gradient} = \\frac{\\partial \\text{Loss}}{\\partial w}\n$$\n\nIt tells us:\n\n> ‚ÄúIf I change this weight a little, how will the loss change?‚Äù  \n\n\n# üßÆ Example: Understanding the Gradient\n\nLet‚Äôs take a simple model:\n\n$$\n\\text{prediction} = x \\times w\n$$\n\nand a **loss function**:\n\n$$\n\\text{Loss} = (prediction - target)^2\n$$\n\n---\n\n### Given:\n- \\( x = 8.5 \\)\n- \\( w = 0.1 \\)\n- \\( target = 0.9 \\)\n\n---\n\n## Step 1: Compute the prediction\n\n$$\nprediction = 8.5 \\times 0.1 = 0.85\n$$\n\n---\n\n## Step 2: Compute the error\n\n$$\nerror = prediction - target = 0.85 - 0.9 = -0.05\n$$\n\n---\n\n## Step 3: Compute the loss (error)\n\n$$\nLoss = (-0.05)^2 = 0.0025\n$$\n\n---\n\n## Step 4: Compute the gradient\n\nUse the chain rule:\n\n$$\n\\frac{\\partial Loss}{\\partial w} = 2 \\times (prediction - target) \\times x\n$$\n\nSubstitute the numbers:\n\n$$\n\\frac{\\partial Loss}{\\partial w} = 2 \\times (-0.05) \\times 8.5 = -0.85\n$$\n\n‚úÖ **Gradient = -0.85**\n\n---\n\n## Step 5: Update the weight (Gradient Descent)\n\nWith learning rate \\( lr = 0.01 \\):\n\n$$\nw_{\\text{new}} = w - lr \\times gradient\n$$\n\n$$\nw_{\\text{new}} = 0.1 - 0.01 \\times (-0.85) = 0.1085\n$$\n\n---\n\n### üß† Interpretation\n\n- The **error** tells us how wrong our prediction is.  \n- The **gradient** tells us **which direction and how strongly** to move `w` to reduce the loss.  \n- The **learning rate** controls how big a step we take in that direction.  \n\n---\n",
      "metadata": {}
    },
    {
      "id": "213dd4cd-ae87-489e-b977-252ca8005807",
      "cell_type": "markdown",
      "source": "## stopping\nthe multiplication by the input does not only scale the error but also is mandatory to prevent the algoithm from taking actions when the input is zero\nso the prediction will always be zero \n\n## üîπ Negative Reversal\n\n**Negative reversal** refers to situations where the **gradient or input changes sign**, and we modify the update rule to handle it.  \n\nIn simpler terms:\n\n- During training, each weight update follows the **gradient** (the direction that reduces the loss).  \n- Sometimes, the gradient suddenly flips from **positive to negative** or vice versa.  \n- If we continue updating blindly, the weight can **overshoot** the minimum, oscillate, or even diverge.  \n\n**Negative reversal methods** detect this sign change and either:\n\n1. **Stop updating** in that direction temporarily.  \n2. **Reduce the step size** (learning rate) to avoid overshooting.  \n3. **Reverse the direction cautiously** to ensure the loss keeps decreasing.\n\n---\n\n### üîπ Why it matters\n\nImagine you are minimizing a function:\n\n$$\nL(w) = (w-3)^2\n$$\n\n- Suppose your current weight is `w = 2.9`.  \n- The gradient is `‚àÇL/‚àÇw = 2*(w-3) = -0.2`.  \n- You update: `w_new = w - lr * gradient`.  \n- Next step, the gradient may flip from negative to positive if the learning rate is too high.  \n- Negative reversal handling **prevents large oscillations** and stabilizes training.\n\n---\n\n### üîπ Analogy\n\nThink of walking downhill (minimizing loss):\n\n- Gradient = slope telling you which way is downhill.  \n- Negative reversal = slope suddenly points uphill.  \n- You **slow down or stop** to avoid overshooting the bottom of the hill.\n\n---\n\n### ‚úÖ Summary\n\n| Term | Meaning |\n|------|---------|\n| Gradient flip / sign change | When ‚àÇLoss/‚àÇw changes sign |\n| Negative reversal | Adjusting updates when this happens |\n| Purpose | Prevent oscillation, overshooting, and divergence |\n",
      "metadata": {}
    },
    {
      "id": "43c5306f-e234-4a6d-aca9-3302a6623079",
      "cell_type": "markdown",
      "source": "### üîπ Better Example of Negative Reversal\n\nSuppose we have a simple **1D loss function**:\n\n$$\nL(w) = (w-3)^2\n$$\n\n- **Goal:** minimize \\(L(w)\\)  \n- **Current weight:** \\(w = 2.9\\)  \n- **Learning rate:** \\(lr = 1.0\\) (intentionally large to illustrate overshooting)  \n\n---\n\n### Step 1: Compute the gradient\n\n$$\n\\frac{\\partial L}{\\partial w} = 2 \\cdot (w - 3) = 2 \\cdot (2.9 - 3) = -0.2\n$$\n\n---\n\n### Step 2: Update the weight (gradient descent)\n\n$$\nw_{\\text{new}} = w - lr \\cdot \\text{gradient} = 2.9 - 1.0 \\cdot (-0.2) = 3.1\n$$\n\n---\n\n### Step 3: Next gradient\n\n$$\n\\frac{\\partial L}{\\partial w} = 2 \\cdot (3.1 - 3) = 0.2\n$$\n\n‚úÖ Notice the **gradient flipped sign** from negative to positive.\n\nWithout negative reversal handling, the next update could **overshoot** again:\n\n$$\nw_{\\text{next}} = 3.1 - 1.0 \\cdot 0.2 = 2.9\n$$\n\nThis causes the weight to **oscillate indefinitely** between 2.9 and 3.1.\n\n---\n\n### Step 4: Apply negative reversal handling\n\n- Detect that the gradient **changed sign**.  \n- Options to stabilize:\n  1. **Stop updating this step** ‚Üí weight stays at 3.1  \n  2. **Reduce learning rate** ‚Üí smaller step prevents overshooting  \n  3. **Reverse cautiously** ‚Üí take a smaller step in the new direction  \n\nResult: weight converges to 3 **without oscillation**.\n\n---\n\n### üîπ Intuition\n\n- Gradient flip = slope suddenly points uphill.  \n- Negative reversal = slow down or pause when slope flips.  \n- Purpose: **prevent overshooting and make training stable**.\n",
      "metadata": {}
    },
    {
      "id": "492b569f-581e-4a3b-a2f9-61d13dfd8683",
      "cell_type": "markdown",
      "source": "# üßÆ Formalizing Error and Gradient Descent\n\n## **1. Defining the Error**\n\nThe error is defined as the **squared difference** between the prediction and the goal:\n\n$$\n\\text{Error} = (pred - pred_{goal})^2\n$$\n\nSquaring the difference:\n\n- Keeps the error **positive**.  \n- **Penalizes larger errors** more strongly.\n\n---\n\n## **2. Deriving the Update with Gradient Descent**\n\nTo **minimize the error**, we need the **slope (derivative)** of the error with respect to the weight.\n\n### **Using the Chain Rule**\n\n$$\n\\frac{\\partial Error}{\\partial weight} = \\frac{\\partial Error}{\\partial pred} \\times \\frac{\\partial pred}{\\partial weight}\n$$\n\n---\n\n### **a. Derivative of Error with respect to Prediction**\n\n$$\n\\frac{\\partial}{\\partial pred}(pred - pred_{goal})^2 = 2(pred - pred_{goal})\n$$\n\n---\n\n### **b. Derivative of Prediction with respect to Weight**\n\nIn a simple **linear model**,  \n\n$$\npred = weight \\times input\n$$\n\nSo,\n\n$$\n\\frac{\\partial pred}{\\partial weight} = input\n$$\n\n---\n\n## **3. Combining Terms to Update the Weight**\n\nNow, combining both derivatives:\n\n$$\n\\frac{\\partial Error}{\\partial weight} = 2(pred - pred_{goal}) \\times input\n$$\n\n---\n\n### **Gradient Descent Update Rule**\n#### to calculate the new weight we have to move in the opposite direction of the gradient\nTo move toward lower error, update the weight in the **opposite direction** of the gradient, scaled by the **learning rate** \\( \\alpha \\):\n\n$$\nweight = weight - \\alpha \\times \\frac{\\partial Error}{\\partial weight}\n$$\n\nSubstituting the gradient:\n\n$$\nweight = weight - \\alpha \\times 2(pred - pred_{goal}) \\times input\n$$\n\n---\n\n## **4. Summary**\n\n| Concept | Explanation |\n|----------|--------------|\n| **Error Function** | Measures the squared difference between prediction and goal. |\n| **Derivative (Gradient)** | Quantifies how much the error changes with respect to the weight. |\n| **Input Factor** | Appears because prediction depends linearly on input in \\( pred = weight \\times input \\). |\n| **Learning Rate \\( \\alpha \\)** | Controls the step size for updates ‚Äî too large causes instability, too small slows learning. |\n| **\"Hot and Cold\" Learning** | Conceptually: adjust the weight in the direction that reduces error. |\n\n",
      "metadata": {}
    },
    {
      "id": "e467b3cf-caa8-49a8-989f-855a604945a0",
      "cell_type": "code",
      "source": "weight = 0.5\ngoal_pred = 0.8\n\ninput = 2\nalpha =0.1\nfor iteration in range(20):\n    pred = input * weight\n    error = (pred-goal_pred)**2\n    delta = 2* (pred-goal_pred) * input\n    # we replace pred by input * weight we get \n    weight = weight - alpha * delta\n    print(\"error:\" + str(error) + \" prediction:\" +str(pred))",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "error:0.03999999999999998 prediction:1.0\nerror:0.001600000000000003 prediction:0.8400000000000001\nerror:6.400000000000012e-05 prediction:0.808\nerror:2.5600000000001466e-06 prediction:0.8016000000000001\nerror:1.0239999999999165e-07 prediction:0.80032\nerror:4.095999999993982e-09 prediction:0.800064\nerror:1.6384000000089615e-10 prediction:0.8000128000000001\nerror:6.553599999922159e-12 prediction:0.80000256\nerror:2.6214399995141164e-13 prediction:0.800000512\nerror:1.0485760007151412e-14 prediction:0.8000001024000001\nerror:4.1943040119555116e-16 prediction:0.8000000204800001\nerror:1.6777215865923107e-17 prediction:0.800000004096\nerror:6.710887073965021e-19 prediction:0.8000000008192001\nerror:2.6843562847778015e-20 prediction:0.8000000001638401\nerror:1.0737425139111206e-21 prediction:0.800000000032768\nerror:4.295028263521311e-23 prediction:0.8000000000065537\nerror:1.7180113054085245e-24 prediction:0.8000000000013108\nerror:6.870881109457025e-26 prediction:0.8000000000002622\nerror:2.746024811074342e-27 prediction:0.8000000000000524\nerror:1.0891210872707594e-28 prediction:0.8000000000000105\n"
        }
      ],
      "execution_count": 3
    },
    {
      "id": "6a72f4f4-8f98-4b3f-acff-63963dc18926",
      "cell_type": "markdown",
      "source": "# chapter 5",
      "metadata": {}
    },
    {
      "id": "3e586678-d450-4ecd-9b2d-b977bcfdfbe8",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}